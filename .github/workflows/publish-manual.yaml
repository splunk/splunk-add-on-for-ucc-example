# jscpd:ignore-start
name: test-workflow
on: 
  workflow_dispatch:
    inputs:
      SPLUNK_VERSION:
        default: '8.2.2107'
        description: 'SPLUNK_VERSION'
      TEST_TYPE:
        description: 'Test type'     
        default: 'knowledge'
        required: true
        type: choice
        options:
        - knowledge
        - scripted-input
      OS: 
        description: 'OS Version to test with scripted-input tests. e.g ["ubuntu:16.04","ubuntu:18.04","ubuntu:22.04"]'
        default: 'ubuntu:16.04'
      SC4S_VERSION:
        description: 'SC4S version that should be used for addon test-executions'
        default: 'latest'
      SC4S_REGISTRY:
        description: "SC4S registry that should be used for installing the above SC4S version"
        default: 'ghcr.io/splunk/splunk-connect-for-syslog/container2'
        type: choice
        options:
        - splunk/scs
        - ghcr.io/splunk/splunk-connect-for-syslog/container
        - ghcr.io/splunk/splunk-connect-for-syslog/container2
      ADDITIONAL_FILTER:
        description: 'Additional filter for pytest command, e.g. -k test_splunk_internal_errors'

jobs:
  build:
    name: build
    runs-on: ubuntu-latest
    outputs:
      buildname: ${{ steps.buildupload.outputs.name }}
    steps:
      - uses: actions/checkout@v3
        with:
          # Very Important semantic-release won't trigger a tagged
          # build if this is not set false
          persist-credentials: false
      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: 3.7
      - uses: actions/setup-node@v3
        with:
          node-version: 14
      - name: create requirements file for pip
        run: |
          if [ -f "poetry.lock" ]
          then
            echo " potery.lock found "
            sudo pip3 install poetry
            mkdir -p package/lib || true
            poetry export --without-hashes -o package/lib/requirements.txt
            poetry export --without-hashes --dev -o requirements_dev.txt
            cat requirements_dev.txt
          fi
      - name: Get pip cache dir
        id: pip-cache
        run: |
          echo "::set-output name=dir::$(pip cache dir)"
      - name: Run Check there are libraries to scan
        id: checklibs
        run: if [ -f requirements_dev.txt ]; then echo "::set-output name=ENABLED::true"; fi
      - name: pip cache
        if: ${{ steps.checklibs.outputs.ENABLED == 'true' }}
        uses: actions/cache@v3
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements_dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install deps
        if: ${{ steps.checklibs.outputs.ENABLED == 'true' }}
        run: pip install -r requirements_dev.txt
      - name: Semantic Release Get Next
        id: semantic
        if: github.event_name != 'pull_request'
        uses: cycjimmy/semantic-release-action@v2.7.0
        with:
          semantic_version: 17
          extra_plugins: |
            @semantic-release/exec
            @semantic-release/git
          dry_run: true
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_ADMIN }}
      - name: Determine the version to build
        id: BuildVersion
        uses: splunk/addonfactory-get-splunk-package-version-action@v1
        with:
          SemVer: ${{ steps.semantic.outputs.new_release_version }}
          PrNumber: ${{ github.event.number }}
      - name: Download THRIDPARTY
        if: github.event_name != 'pull_request' && github.event_name != 'schedule'
        uses: actions/download-artifact@v3
        with:
          name: THIRDPARTY
      - name: Download THRIDPARTY (Optional for PR and schedule)
        if: github.event_name == 'pull_request' || github.event_name == 'schedule'
        continue-on-error: true
        uses: actions/download-artifact@v3
        with:
          name: THIRDPARTY
      - name: Build Package
        id: uccgen
        uses: splunk/addonfactory-ucc-generator-action@v1
        with:
          version: ${{ steps.BuildVersion.outputs.VERSION }}
      - name: Slim Package
        id: slim
        uses: splunk/addonfactory-packaging-toolkit-action@v1
        with:
          source: ${{ steps.uccgen.outputs.OUTPUT }}
      - name: artifact-splunk-unpacked
        uses: actions/upload-artifact@v3
        with:
          name: package-raw
          path: ${{ steps.uccgen.outputs.OUTPUT }}**
        if: always()
      - name: artifact-splunk-base
        uses: actions/upload-artifact@v3
        with:
          name: package-splunkbase
          path: ${{ steps.slim.outputs.OUTPUT }}
        if: always()
      - name: upload-build-to-s3
        id: buildupload
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "::set-output name=name::$(basename "${{ steps.slim.outputs.OUTPUT }}")"
          basename "${{ steps.slim.outputs.OUTPUT }}"
          aws s3 cp "${{ steps.slim.outputs.OUTPUT }}" s3://ta-staging-artifacts/ta-apps/
      - name: artifact-splunk-parts
        uses: actions/upload-artifact@v3
        with:
          name: package-deployment
          path: build/package/deployment**
        if: always()

  test-inventory:
    runs-on: ubuntu-latest
    # Map a step output to a job output
    outputs:
      knowledge: ${{ steps.testset.outputs.knowledge }}
      scripted_inputs: ${{ steps.testset.outputs.scripted_inputs }}
    steps:
      - uses: actions/checkout@v3
      - id: testset
        name: testsets
        run: |
          find tests -type d -maxdepth 1 -mindepth 1 | sed 's|^tests/||g' |  while read -r TESTSET; do echo "::set-output name=$TESTSET::true"; echo "$TESTSET::true"; done


  setup:
    needs:
      - build
      - test-inventory
    runs-on: ubuntu-18.04
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    outputs:
      argo-server: ${{ steps.test-setup.outputs.argo-server }}
      argo-http1: ${{ steps.test-setup.outputs.argo-http1 }}
      argo-secure: ${{ steps.test-setup.outputs.argo-secure }}
      argo-href: ""
      argo-base-href: ${{ steps.test-setup.outputs.argo-base-href }}
      argo-workflow-tmpl-name: ${{ steps.test-setup.outputs.argo-workflow-tmpl-name }}
      argo-namespace: ${{ steps.test-setup.outputs.argo-namespace }}
      addon-name: ${{ steps.test-setup.outputs.addon-name }}
      job-name: ${{ steps.test-setup.outputs.job-name }}
      labels: ${{ steps.test-setup.outputs.labels }}
      addon-upload-path: ${{ steps.test-setup.outputs.addon-upload-path }}
      directory-path: ${{ steps.test-setup.outputs.directory-path }}
      s3-bucket: ${{ steps.test-setup.outputs.s3-bucket }}
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: setup for test
        id: test-setup
        shell: bash
        run: |
          echo "::set-output name=argo-server::argo.staging.wfe.splgdi.com:443"
          echo "::set-output name=argo-http1::true"
          echo "::set-output name=argo-secure::true"
          echo "::set-output name=argo-base-href::\'\'"
          echo "::set-output name=argo-namespace::workflows"
          echo "::set-output name=argo-workflow-tmpl-name::ta-workflow-validate-scripted"

          ADDON_NAME=$(crudini --get package/default/app.conf id name | tr '[:lower:]' '[:upper:]')
          if [[ -n $(echo "${ADDON_NAME}" | awk -F 'SPLUNK_TA_' '{print $2}') ]];
          then
              ADDON_NAME=$(echo "${ADDON_NAME}" | awk -F 'SPLUNK_TA_' '{print $2}')
          elif [[ -n $(echo "${ADDON_NAME}" | awk -F '_FOR_SPLUNK' '{print $1}') ]];
          then
              ADDON_NAME=$(echo "${ADDON_NAME}" | awk -F '_FOR_SPLUNK' '{print $1}')
          fi
          echo "::set-output name=addon-name::\"$ADDON_NAME\""

          JOB_NAME=$(echo "$ADDON_NAME" | tail -c 16)-$(echo "${GITHUB_SHA}" | tail -c 8)-TEST-TYPE-${GITHUB_RUN_ID}
          JOB_NAME=${JOB_NAME//[_.]/-}
          echo "::set-output name=job-name::wf-$JOB_NAME"

          LABELS="addon-name=${ADDON_NAME}"
          echo "::set-output name=labels::$LABELS"

          ADDON_UPLOAD_PATH="s3://ta-staging-artifacts/ta-apps/${{ needs.build.outputs.buildname }}"
          echo "::set-output name=addon-upload-path::$ADDON_UPLOAD_PATH"
          echo "::set-output name=directory-path::/tmp"
          echo "::set-output name=s3-bucket::ta-staging-artifacts"

  run-knowledge-tests:
    if: ${{ needs.test-inventory.outputs.knowledge == 'true' }} && ${{ inputs.TEST_TYPE == 'knowledge' }}
    needs:
      - build
      - test-inventory
      - setup
    runs-on: ubuntu-18.04
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      SC4S_REGISTRY: ${{ inputs.SC4S_REGISTRY }}
      SC4S_VERSION: ${{ inputs.SC4S_VERSION }}
      SPLUNK: ${{ inputs.SPLUNK_VERSION }}
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ inputs.SPLUNK_VERSION }}
      TEST_TYPE: "knowledge"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"

      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@ci/update-scripted-inputs
        with:
          splunk: ${{ env.SPLUNK }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ${{ env.ADDITIONAL_FILTER }}
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: "No"
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          sc4s-version: ${{ env.SC4S_VERSION }}
          sc4s-docker-registry: ${{ env.SC4S_REGISTRY }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        shell: bash
        if: always()
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ env.SPLUNK }} ${{ env.TEST_TYPE }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ env.SPLUNK }} ${{ env.TEST_TYPE }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Upload cim-compliance-report for ${{ env.SPLUNK }}
        uses: actions/upload-artifact@v3
        if: ${{ env.SPLUNK.islatest == true }}
        with:
          name: cim-compliance-report
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results/cim-compliance-report.md
      - name: Upload cim-field-report for ${{ env.SPLUNK }}
        uses: actions/upload-artifact@v3
        if: ${{ env.SPLUNK.islatest == true }}
        with:
          name: cim-field-report
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results/cim_field_report.json
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ env.SPLUNK }} ${{ env.TEST_TYPE }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-scripted-input-tests:
    if: ${{ needs.test-inventory.outputs.scripted_inputs == 'true' }} && ${{ inputs.TEST_TYPE == 'scripted-input' }}
    needs:
      - build
      - test-inventory
      - setup
    runs-on: ubuntu-18.04
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      SPLUNK: ${{ inputs.SPLUNK_VERSION }}
      OS: ${{ inputs.OS }}
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ inputs.SPLUNK_VERSION }}
      TEST_TYPE: "scripted_inputs"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: get os name and version
        id: os-name-version
        shell: bash
        run: |
          OS_NAME_VERSION=${{ env.OS }}
          OS_NAME_VERSION=(${OS_NAME_VERSION//:/ })
          OS_NAME=${OS_NAME_VERSION[0]}
          OS_VERSION=${OS_NAME_VERSION[1]}
          echo "::set-output name=os-name::$OS_NAME"
          echo "::set-output name=os-version::$OS_VERSION"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@ci/update-scripted-inputs
        with:
          splunk: ${{ env.SPLUNK }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: "--uf-host=spl --uf-os=${{ steps.os-name-version.outputs.os-name }} --uf-os-version=${{ steps.os-name-version.outputs.os-version }} -m script_input"
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: "No"
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          os-name: ${{ steps.os-name-version.outputs.os-name }}
          os-version: ${{ steps.os-name-version.outputs.os-version }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ env.SPLUNK }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ env.SPLUNK }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ env.SPLUNK }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }}  test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit
